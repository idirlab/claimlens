{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Eval dataset from politifact data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def scrape_urls(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find the div with id='sources'\n",
    "        sources_div = soup.find('section', id='sources')\n",
    "        \n",
    "        if sources_div:\n",
    "            # Find all anchor tags within the div\n",
    "            links = sources_div.find_all('a')\n",
    "            \n",
    "            # Extract the URLs\n",
    "            urls = [link.get('href') for link in links]\n",
    "            \n",
    "            # Filter out the URLs that are not relevant\n",
    "            urls = [url for url in urls if url and urlparse(url).netloc.endswith('.gov')]\n",
    "            \n",
    "            return urls\n",
    "        else:\n",
    "            print(\"No div with id='sources' found on the page.\")\n",
    "            return []\n",
    "    else:\n",
    "        print(\"Failed to fetch the page:\", response.status_code)\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "politifact_dataset_path = './data/entered_claims 3.csv'\n",
    "\n",
    "politifact_data = pd.read_csv(politifact_dataset_path)\n",
    "\n",
    "politifact_urls = politifact_data.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gov_links = [scrape_urls(x) for x in politifact_urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, save the data to a JSON file\n",
    "\n",
    "# import json\n",
    "\n",
    "# with open('./data/gov_links.json', 'w') as f:\n",
    "#     json.dump(gov_links, f)\n",
    "\n",
    "\n",
    "# Optionally, load the data from a JSON file\n",
    "\n",
    "# import json\n",
    "\n",
    "# with open('./data/gov_links.json', 'r') as f:\n",
    "#     gov_links = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Create a map to map the bill name to the appropriate abbreviation\n",
    "bill_name_map = {\n",
    "    'house-bill': 'HR',\n",
    "    'senate-bill': 'S',\n",
    "    'house-resolution': 'HRES',\n",
    "    'senate-resolution': 'SRES',\n",
    "    'house-concurrent-resolution': 'HCONRES',\n",
    "    'senate-concurrent-resolution': 'SCONRES',\n",
    "    'senate-joint-resolution': 'SJRES',\n",
    "    'house-joint-resolution': 'HJRES',\n",
    "}\n",
    "\n",
    "# Create an empty list to store the evaluation set\n",
    "eval_set = []\n",
    "\n",
    "# Iterate over the each politifact URL\n",
    "for i in range(len(gov_links)):\n",
    "    \n",
    "    # Get the list of .gov links for the current politifact URL\n",
    "    poli_sources = gov_links[i]\n",
    "\n",
    "    # Iterate over the list of URLs\n",
    "    for source_url in poli_sources:\n",
    "        \n",
    "        # Parse the URL\n",
    "        parsed_url = urlparse(source_url)\n",
    "\n",
    "        # For now, only supporting congress.gov URLs\n",
    "        if 'congress.gov' in parsed_url.netloc:\n",
    "            \n",
    "            # Normalize and extract the path\n",
    "            path = parsed_url.path if parsed_url.path[0] == '/' else '/' + parsed_url.path\n",
    "            path_parts = path.split('/')\n",
    "            \n",
    "            # Check if the URL is a bill URL\n",
    "            if path_parts[1] != 'bill':\n",
    "                continue\n",
    "            \n",
    "            # Extract the congress number, split by '-' and find first number in e.g., '116th-congress'\n",
    "            congress_number = path_parts[2]\n",
    "            congress_number = congress_number.split('-')[0]\n",
    "            congress_number = re.search(r'\\d+', congress_number).group(0)\n",
    "            \n",
    "            # Extract the bill name (e.g., 'house-bill')\n",
    "            bill_name = path_parts[3]\n",
    "            \n",
    "            # Check if the bill name is in the map, may need to support more in the future?\n",
    "            if not bill_name in bill_name_map:\n",
    "                continue\n",
    "            \n",
    "            # Map the bill name to the appropriate abbreviation\n",
    "            bill_name = bill_name_map[bill_name]\n",
    "\n",
    "            # Extract the bill number. will be first (and hopefully only number in the path), e.g. '1234'\n",
    "            bill_number = path_parts[4]\n",
    "            bill_number = re.search(r'\\d+', bill_number).group(0)\n",
    "\n",
    "            # Add the URL and the bill information to the evaluation set\n",
    "            eval_set += [(politifact_urls[i], f'{congress_number} {bill_name} {bill_number}')]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_bills = pd.DataFrame(eval_set).groupby(0).agg(lambda x: x.tolist()).reset_index().rename(columns={0: 'url', 1: 'sourced_bills'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = politifact_data.merge(grouped_bills, on='url').drop(columns=['Unnamed: 6', 'bills_found', 'actual_bills', 'intersection'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset.to_csv('./data/politifact-claims.csv', index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
